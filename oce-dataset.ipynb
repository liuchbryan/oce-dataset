{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASOS Digital Experiments Datasets - Experiments\n",
    "\n",
    "This notebook contains the code required to reproduce the experiments using the ASOS Digital Experiments Dataset, as described in Section 5 of the paper Datasets for Online Controlled Experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ccce4f4a-d090-4639-bb11-a89b74ffdf89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "sns.set_theme(style=\"ticks\", palette=sns.color_palette(\"Set2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f37708e0-04c0-4a5a-8b7c-c9fea7c8bc93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specifying the directories and paths\n",
    "working_folder = \".\"\n",
    "\n",
    "figures_folder = working_folder + os.path.sep + 'figures' + os.path.sep\n",
    "if not os.path.isdir(figures_folder):\n",
    "    os.mkdir(figures_folder)\n",
    "    \n",
    "data_folder = working_folder + os.path.sep + 'data' + os.path.sep\n",
    "\n",
    "abtest_metrics_local_path = data_folder + 'asos_digital_experiments_dataset.parquet'\n",
    "abtest_metrics_remote_path = \"https://osf.io/62t7f/download\"\n",
    "\n",
    "# Download the dataset if it does not already exist\n",
    "# The body of the if-statement is a shell command\n",
    "if not os.path.exists(abtest_metrics_local_path):\n",
    "    !wget -O $abtest_metrics_local_path $abtest_metrics_remote_path\n",
    "\n",
    "# Load the dataset\n",
    "abtest_metrics_df = pd.read_parquet(abtest_metrics_local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalized_effect_size(mean_x: float, mean_y: float):\n",
    "    return (mean_y - mean_x)\n",
    "\n",
    "def cohens_d(\n",
    "    mean_x: float, mean_y: float, variance_x: float, \n",
    "    variance_y: float, count_x: float, count_y: float):\n",
    "    return (\n",
    "        (mean_y - mean_x) /\n",
    "        np.sqrt(((count_x - 1) * variance_x + (count_y - 1) * variance_y) /\n",
    "                (count_x + count_y - 2))\n",
    "    )\n",
    "\n",
    "def bayesian_effect_size(\n",
    "    mean_x: float, mean_y: float, variance_x: float, \n",
    "    variance_y: float, count_x: float, count_y: float):\n",
    "    \n",
    "    return(\n",
    "        (mean_y - mean_x) /\n",
    "        np.sqrt((variance_x / count_x + variance_y / count_y) /\n",
    "                (1.0 / count_x + 1.0 / count_y))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mSPRT_vanilla_normal_p_value_aux(\n",
    "    mean_x: float, mean_y: float, variance_x: float, \n",
    "    variance_y: float, count_x: float, count_y: float, \n",
    "    theta_0: float = 0, tau_sq: float = 0.0001):\n",
    "    \n",
    "    if (count_x == 0) or (count_y == 0):\n",
    "        return 1.0\n",
    "\n",
    "    count_mean = 2 / (1/count_x + 1/count_y)\n",
    "\n",
    "    test_statistic = (\n",
    "      np.sqrt((variance_x + variance_y)/\n",
    "              (variance_x + variance_y + count_mean * tau_sq)) *\n",
    "      np.exp((count_mean ** 2.0 * tau_sq *\n",
    "              (mean_y - mean_x - theta_0) ** 2.0) /\n",
    "             (2.0 * (variance_x + variance_y) *\n",
    "              (variance_x + variance_y + count_mean * tau_sq)))\n",
    "    )\n",
    "    return 1.0 / max(1.0, test_statistic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_factor(\n",
    "    mean_x: float, mean_y: float, variance_x: float, \n",
    "    variance_y: float, count_x: float, count_y: float, \n",
    "    theta_0: float = 0, V_sq: float = 0.0001):\n",
    "    \n",
    "    if (count_x == 0) or (count_y == 0):\n",
    "        return 1.0\n",
    "    \n",
    "    effect_size = (\n",
    "        bayesian_effect_size(\n",
    "            mean_x=mean_x, mean_y=mean_y, variance_x=variance_x, \n",
    "            variance_y=variance_y, count_x=count_x, count_y=count_y))\n",
    "    \n",
    "    effective_sample_size_inverse = 1.0 / count_x + 1.0 / count_y\n",
    "    \n",
    "    # Return the Bayes factor, which is the likelihood of \n",
    "    # the calculated effect size under H_1 over that of H_0\n",
    "    return (\n",
    "        norm.pdf(effect_size, theta_0, \n",
    "                 np.sqrt(V_sq + effective_sample_size_inverse)) /\n",
    "        norm.pdf(effect_size, theta_0, \n",
    "                 np.sqrt(effective_sample_size_inverse))\n",
    "    )\n",
    "\n",
    "def bayesian_posterior_H0_probability(\n",
    "    mean_x: float, mean_y: float, variance_x: float, \n",
    "    variance_y: float, count_x: float, count_y: float, \n",
    "    theta_0: float = 0, V_sq: float = 0.0001,\n",
    "    prior_H0_probability: float = 0.75):\n",
    "    \n",
    "    prior_odds = (1.0 - prior_H0_probability) / prior_H0_probability\n",
    "    bf = bayes_factor(mean_x=mean_x, mean_y=mean_y, \n",
    "                      variance_x=variance_x, variance_y=variance_y, \n",
    "                      count_x=count_x, count_y=count_y, \n",
    "                      theta_0=theta_0, V_sq=V_sq)\n",
    "    \n",
    "    posterior_odds = bf * prior_odds\n",
    "    \n",
    "    return(1 / (posterior_odds + 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3378e87c-a7b3-47c6-884d-7615e81bb565",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dataset descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Experiment count: {len(abtest_metrics_df['experiment_id'].unique())}\")\n",
    "\n",
    "print(f\"Total treatment count: {len(abtest_metrics_df[['experiment_id', 'variant_id']].drop_duplicates())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the joint keys for rows that corresponds to the\n",
    "# final record of the experiment\n",
    "experiment_keys_overall_df = \\\n",
    "abtest_metrics_df \\\n",
    ".groupby(['experiment_id', 'variant_id', 'metric_id'])\\\n",
    ".agg({'time_since_start':'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size_df = (\n",
    "  experiment_keys_overall_df\n",
    "  .reset_index()\n",
    "  .merge(abtest_metrics_df, how='inner')\n",
    ")\n",
    "\n",
    "effect_size_df['unnormalized_effect_size'] = (\n",
    "  effect_size_df.apply(\n",
    "    lambda row: unnormalized_effect_size(\n",
    "      mean_x=row['mean_c'], mean_y=row['mean_t']),\n",
    "    axis=1)\n",
    ")\n",
    "\n",
    "effect_size_df['cohens_d'] = (\n",
    "  effect_size_df.apply(\n",
    "    lambda row: cohens_d(\n",
    "      mean_x=row['mean_c'], mean_y=row['mean_t'],\n",
    "      variance_x=row['variance_c'], variance_y=row['variance_t'],\n",
    "      count_x=row['count_c'], count_y=row['count_t']),\n",
    "    axis=1)\n",
    ")\n",
    "\n",
    "effect_size_df['bayesian_effect_size'] = (\n",
    "  effect_size_df.apply(\n",
    "    lambda row: bayesian_effect_size(\n",
    "      mean_x=row['mean_c'], mean_y=row['mean_t'],\n",
    "      variance_x=row['variance_c'], variance_y=row['variance_t'],\n",
    "      count_x=row['count_c'], count_y=row['count_t']),\n",
    "    axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of effect size - Cohen's $d$\n",
    "\n",
    "$$d = \\left(\\bar{Y} - \\bar{X}\\right) \\bigg/\n",
    "    \\sqrt{\\frac{(N - 1) s^2_X + (M - 1) s^2_Y}{N + M - 2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_id in effect_size_df['metric_id'].unique():\n",
    "    \n",
    "    cohens_d_data = (\n",
    "        effect_size_df[effect_size_df['metric_id'] == metric_id]['cohens_d'])\n",
    "    \n",
    "    print(f'Metric {metric_id} - Sample variance: {np.var(cohens_d_data)}')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    sns.histplot(ax=ax,\n",
    "                 data=cohens_d_data,\n",
    "                 bins=np.linspace(-0.02,0.02,21))\n",
    "    #axlabel='Metric: '+ str(metric_id)\n",
    "#     ax.set_xlim(0, 1)\n",
    "#     ax.set_ylim(0, 28)\n",
    "    ax.set_xlabel(None)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_title(f\"Metric {metric_id}\")\n",
    "\n",
    "    fig.savefig(figures_folder + 'cohens_d_metric_' + str(metric_id) + '.pdf',\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of effect size - Bayesian effect size\n",
    "\n",
    "$$\\frac{\\bar{Y}_m - \\bar{X}_n}{\\sqrt{\\big(\\frac{\\sigma^2_X}{n} + \\frac{\\sigma^2_Y}{m}\\big) / \\left(\\frac{1}{n}+ \\frac{1}{m}\\right)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_id in effect_size_df['metric_id'].unique():\n",
    "    \n",
    "    bayesian_effect_size_data = (\n",
    "        effect_size_df[effect_size_df['metric_id'] == metric_id]\n",
    "        ['bayesian_effect_size'])\n",
    "    \n",
    "    print(f'Metric {metric_id} - Sample variance: {np.var(bayesian_effect_size_data)}')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    sns.histplot(ax=ax,\n",
    "                 data=bayesian_effect_size_data,\n",
    "                 bins=np.linspace(-0.02,0.02,21))\n",
    "    #axlabel='Metric: '+ str(metric_id)\n",
    "#     ax.set_xlim(0, 1)\n",
    "#     ax.set_ylim(0, 28)\n",
    "    ax.set_xlabel(None)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_title(f\"Metric {metric_id}\")\n",
    "\n",
    "    fig.savefig(figures_folder + 'bayesian_effect_size_metric_' + str(metric_id) + '.pdf',\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of p-values under Welch's t-test\n",
    "\n",
    "Code leading to Figure 1 in Page 8 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2520c78-6f98-4550-b3f5-620ab8b28758",
     "showTitle": true,
     "title": "Run 2-tailed Welch's t-test and obtain p-value"
    }
   },
   "outputs": [],
   "source": [
    "t_test_data_df = (\n",
    "  experiment_keys_overall_df\n",
    "  .reset_index()\n",
    "  .merge(abtest_metrics_df, how='inner')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c3fd7b1-87e6-4469-b6c3-2c639c10dca3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ttest_lambda = (\n",
    "    lambda x: scipy.stats.ttest_ind_from_stats(\n",
    "        x['mean_c'],np.sqrt(x['variance_c']),\n",
    "        x['count_c'], x['mean_t'],\n",
    "        np.sqrt(x['variance_t']), x['count_t'],\n",
    "        equal_var=False)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21747afa-e3e3-45d0-b343-24d9751a4c2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "t_test_data_df['t-test-p-value'] = (\n",
    "    t_test_data_df.apply(ttest_lambda, axis=1))\n",
    "\n",
    "abtest_metrics_df = (\n",
    "    abtest_metrics_df.merge(\n",
    "        t_test_data_df[['experiment_id', 'variant_id', \n",
    "                        'metric_id','t-test-p-value']], \n",
    "        on=['experiment_id', 'variant_id', 'metric_id'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "48cc0e99-4774-4772-8eb7-9777afc0fe2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for metric_id in t_test_data_df['metric_id'].unique():\n",
    "    print('metric_id', metric_id)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(2, 1.7))\n",
    "    sns.histplot(ax=ax,\n",
    "                data=t_test_data_df[t_test_data_df['metric_id'] == metric_id]\n",
    "                                   ['t-test-p-value'],\n",
    "                bins=np.linspace(0,1,21))\n",
    "    #axlabel='Metric: '+ str(metric_id)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 28)\n",
    "    ax.set_xlabel(None)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_title(f\"Metric {metric_id}\")\n",
    "\n",
    "    fig.savefig(figures_folder + 't-test_p-value_metric_' + str(metric_id) + '.pdf',\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change in mSPRT p-values / Bayesian posterior belief in null \n",
    "\n",
    "Code leading to Figure 2 (Page 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce629e6b-539d-42e2-82ab-905d23e1e369",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate mSPRT p-value for ALL rows\n",
    "\n",
    "# One tau_sq for each metric\n",
    "# The naive tau_sq estimate is the sample variance\n",
    "# of Cohen's d calculated above\n",
    "# Metric 1 - Sample variance: 1.301572657324043e-05\n",
    "# Metric 2 - Sample variance: 1.0651973563344495e-05\n",
    "# Metric 3 - Sample variance: 6.478802824829137e-06\n",
    "# Metric 4 - Sample variance: 5.915041799704489e-06\n",
    "naive_tau_sq = {\n",
    "    1: 1.30e-05,\n",
    "    2: 1.07e-05,\n",
    "    3: 6.48e-06,\n",
    "    4: 5.92e-06,\n",
    "}\n",
    "\n",
    "abtest_metrics_df['mSPRT-vanilla-p-value-aux'] = (\n",
    "  abtest_metrics_df.apply(\n",
    "    lambda row: mSPRT_vanilla_normal_p_value_aux(\n",
    "      mean_x=row['mean_c'], mean_y=row['mean_t'],\n",
    "      variance_x=row['variance_c'], variance_y=row['variance_t'],\n",
    "      count_x=row['count_c'], count_y=row['count_t'],\n",
    "      theta_0=0, tau_sq=naive_tau_sq[row['metric_id']] * row['variance_c']),\n",
    "    axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9347569b-2dcd-424e-aa23-967d5dfa7ae6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "abtest_metrics_df = (\n",
    "    abtest_metrics_df.sort_values(\n",
    "        ['experiment_id', 'variant_id', 'metric_id']))\n",
    "\n",
    "# Apply the streming definition of a mSPRT p-value\n",
    "abtest_metrics_df['mSPRT-vanilla-p-value'] = (\n",
    "  abtest_metrics_df\n",
    "  .groupby(['experiment_id', 'variant_id', 'metric_id'])\n",
    "  ['mSPRT-vanilla-p-value-aux']\n",
    "  .transform(lambda row: row.expanding(min_periods=2).min())\n",
    ")\n",
    "\n",
    "abtest_metrics_df['mSPRT-vanilla-p-value'] = (\n",
    "  abtest_metrics_df['mSPRT-vanilla-p-value'].fillna(1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Bayesian A/B test Bayes Factor + \n",
    "# Posterior null probability (P(H_0 is true | observation)) for ALL rows \n",
    "\n",
    "# One V_sq estimate for each metric\n",
    "# The naive V_sq estimate takes the sample variance of\n",
    "# Bayesian effect size calculated above\n",
    "# Metric 1 - Sample variance: 1.3020009248594856e-05\n",
    "# Metric 2 - Sample variance: 1.0661803999734505e-05\n",
    "# Metric 3 - Sample variance: 6.490907690571599e-06\n",
    "# Metric 4 - Sample variance: 5.931773520743273e-06\n",
    "naive_V_sq = {\n",
    "    1: 1.30e-05,\n",
    "    2: 1.07e-05,\n",
    "    3: 6.49e-06,\n",
    "    4: 5.93e-06,\n",
    "}\n",
    "\n",
    "abtest_metrics_df['bayesian_posterior_H0_probability'] = (\n",
    "    abtest_metrics_df.apply(\n",
    "        lambda row: bayesian_posterior_H0_probability(\n",
    "          mean_x=row['mean_c'], mean_y=row['mean_t'],\n",
    "          variance_x=row['variance_c'], variance_y=row['variance_t'],\n",
    "          count_x=row['count_c'], count_y=row['count_t'],\n",
    "          theta_0=0, V_sq=naive_V_sq[row['metric_id']],\n",
    "          prior_H0_probability=0.75),\n",
    "        axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6dcdda9-c64d-4fd9-a8e0-541b97a5226f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "abtest_metrics_df = (\n",
    "    abtest_metrics_df.merge(\n",
    "        abtest_metrics_df\n",
    "            .groupby(['experiment_id', 'variant_id', 'metric_id'])['time_since_start']\n",
    "            .max()\n",
    "            .reset_index()\n",
    "            .rename(columns={'time_since_start':'design_duration'}),\n",
    "        on=['experiment_id', 'variant_id', 'metric_id'])\n",
    ")\n",
    "\n",
    "# Calculate % experiemnt progress (i.e. time_since_start / max(time_since_start))\n",
    "abtest_metrics_df['time_progress'] = (\n",
    "    abtest_metrics_df['time_since_start'] /\n",
    "    abtest_metrics_df['design_duration'])\n",
    "    \n",
    "abtest_metrics_df['experiment_variant_id'] = (\n",
    "    abtest_metrics_df['experiment_id'].map(str) + '-' + \n",
    "    abtest_metrics_df['variant_id'].map(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abtest_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_experiment_variant_id = [\n",
    "    'c56288-1', 'a4386f-1', 'bac0d3-1', '08bcc2-1', '591c2c-1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca445384-7f09-4741-88a9-458610597fc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for metric_id in abtest_metrics_df['metric_id'].unique():\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(3, 1.8))\n",
    "\n",
    "    sns.lineplot(x=\"time_progress\", y=\"mSPRT-vanilla-p-value\",\n",
    "                 hue=\"experiment_variant_id\",\n",
    "                 data=abtest_metrics_df[\n",
    "                     (abtest_metrics_df.experiment_variant_id.isin(\n",
    "                         target_experiment_variant_id)) & \n",
    "                     (abtest_metrics_df.metric_id==metric_id)],\n",
    "                 ax=ax,\n",
    "                 legend=None)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(-0.02, 1.02)\n",
    "\n",
    "    ax.set_ylabel('mSPRT $p$-value')\n",
    "    ax.set_xlabel(None)\n",
    "\n",
    "    fig.savefig(figures_folder + 'time_progress_mSPRT_metric_' + str(metric_id) + '.pdf',\n",
    "                bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_id in abtest_metrics_df['metric_id'].unique():\n",
    "    fig, ax = plt.subplots(figsize=(3, 1.8))\n",
    "\n",
    "    sns.lineplot(x=\"time_progress\", y=\"bayesian_posterior_H0_probability\",\n",
    "                 hue=\"experiment_variant_id\",\n",
    "                 data=abtest_metrics_df[\n",
    "                     (abtest_metrics_df.experiment_variant_id.isin(\n",
    "                         target_experiment_variant_id)) & \n",
    "                     (abtest_metrics_df.metric_id==metric_id)],\n",
    "                 ax=ax)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(-0.02, 1.02)\n",
    "\n",
    "    ax.set_ylabel('$P(H_0 | Data)$')\n",
    "    ax.set_xlabel(None)\n",
    "    \n",
    "    ax.legend(title='Expt.-Variant ID',\n",
    "              loc='upper left',\n",
    "              bbox_to_anchor=(1, 1))\n",
    "\n",
    "    fig.savefig(figures_folder + 'time_progress_bayesian_metric_' + \n",
    "                str(metric_id) + '.pdf',\n",
    "                bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between Welch's t-test and mSPRT\n",
    "\n",
    "\"Confusion matrices\" between Welch's t-test and mSPRT at the end of eperiment - Table 2 (Page 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abtest_metrics_df['Both significant'] = (\n",
    "    (abtest_metrics_df['t-test-p-value']<=alpha) &\n",
    "    (abtest_metrics_df['mSPRT-vanilla-p-value']<=alpha))\n",
    "abtest_metrics_df['Only mSPRT significant'] = (\n",
    "    (abtest_metrics_df['t-test-p-value']>alpha) & \n",
    "    (abtest_metrics_df['mSPRT-vanilla-p-value']<=alpha))\n",
    "abtest_metrics_df['Both not significant'] = (\n",
    "    (abtest_metrics_df['t-test-p-value']>alpha) & \n",
    "    (abtest_metrics_df['mSPRT-vanilla-p-value']>alpha))\n",
    "abtest_metrics_df['Only t-test significant'] = (\n",
    "    (abtest_metrics_df['t-test-p-value']<=alpha) & \n",
    "    (abtest_metrics_df['mSPRT-vanilla-p-value']>alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(abtest_metrics_df\n",
    " [(abtest_metrics_df['time_progress']==1)]\n",
    " .groupby('metric_id')\n",
    " [['Both significant','Only mSPRT significant',\n",
    "   'Both not significant','Only t-test significant']]\n",
    " .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 10\n",
    "labels=['Both not significant','Only mSPRT significant',\n",
    "        'Only t-test significant','Both significant']\n",
    "abtest_metrics_df['time_progress_bin'] = (\n",
    "    pd.cut(abtest_metrics_df['time_progress'],\n",
    "           np.linspace(0,1,nbins+1), retbins=False))\n",
    "\n",
    "#count snapshots per time_bin\n",
    "abtest_metrics_evol_df = (\n",
    "    abtest_metrics_df\n",
    "        .groupby(['experiment_variant_id','time_progress_bin'])[['time_progress']]\n",
    "        .count().groupby('experiment_variant_id').min()\n",
    "        .rename(columns={'time_progress':'min_samples_in_time_bin'})\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "#filter experiment_variant_id that have at least one entry per time bin\n",
    "abtest_metrics_evol_df = (\n",
    "    abtest_metrics_evol_df\n",
    "        [abtest_metrics_evol_df['min_samples_in_time_bin']>=1]\n",
    "        [['experiment_variant_id']])\n",
    "\n",
    "print('# experiments with at least one entry per time bin', \n",
    "      len(abtest_metrics_evol_df['experiment_variant_id'].unique()))\n",
    "\n",
    "abtest_metric_evol_df = (\n",
    "    abtest_metrics_df[['experiment_variant_id','metric_id','time_progress_bin'] + labels]\n",
    "        .merge(abtest_metrics_evol_df,\n",
    "               on='experiment_variant_id')\n",
    "        .groupby(['time_progress_bin', 'experiment_variant_id', 'metric_id']).last()\n",
    "        .reset_index())\n",
    "\n",
    "abtest_metric_evol_df['time_progress_bin'] = (\n",
    "    [x.right for x in abtest_metric_evol_df['time_progress_bin'].values])\n",
    "\n",
    "abtest_metric_evol_df = (\n",
    "    abtest_metric_evol_df.groupby(['time_progress_bin','metric_id'])\n",
    "    .mean().reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abtest_metric_evol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_id in abtest_metric_evol_df['metric_id'].unique():\n",
    "    print('metric_id', metric_id)\n",
    "    x = abtest_metric_evol_df['time_progress_bin'].unique()\n",
    "    y = abtest_metric_evol_df[abtest_metric_evol_df['metric_id'] == metric_id][labels].T.values\n",
    "\n",
    "    # set seaborn style\n",
    "\n",
    "    # Plot\n",
    "    plt.stackplot(x,y, labels=labels)\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlim(0.1, 1)\n",
    "\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.show()\n",
    "    plt.savefig(figures_folder + 'time_progress_type_metric_' + str(metric_id) + '.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1c21a533-2d5a-4b93-b4d7-45eb8cb673cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Future work:\n",
    "\n",
    "* Confusion matrix between Welch's t-test and Bayesian A/B test\n",
    "* Cumulative plot off type-I, type-II and correct rejection or no-rejection\n",
    "* Show data spike experiment, comment on how it affects mSPRT(vanilla), mSPRT(in deployment) and Bayesian A/B Test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "oce-dataset",
   "notebookOrigID": 1499717207767751,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
