{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ccce4f4a-d090-4639-bb11-a89b74ffdf89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"ticks\", palette=sns.color_palette(\"Set2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f37708e0-04c0-4a5a-8b7c-c9fea7c8bc93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "working_folder = \".\"\n",
    "figures_folder = working_folder + os.path.sep + 'figures' + os.path.sep\n",
    "if not os.path.isdir(figures_folder):\n",
    "    os.mkdir(figures_folder)\n",
    "abtest_metrics_df = pd.read_parquet(working_folder + os.path.sep + 'abtest_metrics_anonymised.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mSPRT_vanilla_normal_p_value_aux(mean_x: float, mean_y: float, variance_x: float, variance_y: \n",
    "                                     float, count_x: float, count_y: float, theta_0: float = 0, tau_sq: float = 0.0001):\n",
    "    \n",
    "    if (count_x == 0) or (count_y == 0):\n",
    "        return 1.0\n",
    "\n",
    "    count_mean = 2 / (1/count_x + 1/count_y)\n",
    "\n",
    "    test_statistic = (\n",
    "      np.sqrt((variance_x + variance_y)/\n",
    "              (variance_x + variance_y + count_mean * tau_sq)) *\n",
    "      np.exp((count_mean ** 2.0 * tau_sq *\n",
    "              (mean_y - mean_x - theta_0) ** 2.0) /\n",
    "             (2.0 * (variance_x + variance_y) *\n",
    "              (variance_x + variance_y + count_mean * tau_sq)))\n",
    "    )\n",
    "    return 1.0 / max(1.0, test_statistic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eee5cb85-f820-4bd2-8f0e-88a8dcf92e86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## quality checks (to delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mSPRT_vanilla_normal_p_value_aux(0.5, 0.506, 0.25, 0.25, 200000, 100000, 0, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d32142d-f7c3-44c9-8543-813b29858e7d",
     "showTitle": true,
     "title": "Check for duplicates"
    }
   },
   "outputs": [],
   "source": [
    "abtest_metrics_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0ede5963-861a-4402-b2f1-a2dc4accdc63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "abtest_metrics_df.drop_duplicates(['experiment_id', 'variant_id', 'metric_id', 'time_since_start']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7fc3c118-3616-4eb8-982c-aa248fdd1bdb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "TODO: Find the 8 duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3378e87c-a7b3-47c6-884d-7615e81bb565",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dataset descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "05983e5d-b5ec-4cf3-bc40-0926fe311dae",
     "showTitle": true,
     "title": "Distribution of effect sizes"
    }
   },
   "source": [
    "### Reproduce the effect size distribution plot in the report \"ASOS experimentation meta-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2520c78-6f98-4550-b3f5-620ab8b28758",
     "showTitle": true,
     "title": "Run 2-tailed Welch's t-test and obtain p-value"
    }
   },
   "outputs": [],
   "source": [
    "experiment_keys_overall_df = \\\n",
    "abtest_metrics_df \\\n",
    ".groupby(['experiment_id', 'variant_id', 'metric_id'])\\\n",
    ".agg({'time_since_start':'max'})\n",
    "\n",
    "t_test_data_df = (\n",
    "  experiment_keys_overall_df\n",
    "  .reset_index()\n",
    "  .merge(abtest_metrics_df, how='inner')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c3fd7b1-87e6-4469-b6c3-2c639c10dca3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ttest_lambda = lambda x: scipy.stats.ttest_ind_from_stats(x['mean_c'],np.sqrt(x['variance_c']),\n",
    "                                                          x['count_c'], x['mean_t'],\n",
    "                                                          np.sqrt(x['variance_t']), x['count_t'],\n",
    "                                                          equal_var=False)[1]\n",
    "\n",
    "#scipy.stats.ttest_ind_from_stats(mean1, std1, nobs1, mean2, std2, nobs2, equal_var=True, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21747afa-e3e3-45d0-b343-24d9751a4c2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "t_test_data_df['t-test-p-value']=t_test_data_df.apply(ttest_lambda, axis=1)\n",
    "abtest_metrics_df = abtest_metrics_df.merge(t_test_data_df[['experiment_id', 'variant_id', 'metric_id','t-test-p-value']], \n",
    "                                            on=['experiment_id', 'variant_id', 'metric_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "48cc0e99-4774-4772-8eb7-9777afc0fe2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for metric_id in t_test_data_df['metric_id'].unique():\n",
    "    print('metric_id', metric_id)\n",
    "    plt.figure()\n",
    "    sns.displot(t_test_data_df[t_test_data_df['metric_id']==metric_id]['t-test-p-value'],bins=np.linspace(0,1,20),rug=False,kde=False)\n",
    "    #axlabel='Metric: '+ str(metric_id)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.show()\n",
    "    plt.savefig(figures_folder + 't-test_p-value_metric_' + str(metric_id) + '.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "58f0680b-1a9f-494e-96dc-711924c004a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Calculate mSPRT p-value for ALL rows  \n",
    "Calculate Bayesian A/B test Bayes Factor + Posterior Odds for ALL rows  \n",
    "Calculate % experiemnt progress (i.e. time_since_start / max(time_since_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce629e6b-539d-42e2-82ab-905d23e1e369",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "abtest_metrics_df['mSPRT-vanilla-p-value-aux'] = (\n",
    "  abtest_metrics_df.apply(\n",
    "    lambda row: mSPRT_vanilla_normal_p_value_aux(\n",
    "      mean_x=row['mean_c'], mean_y=row['mean_t'],\n",
    "      variance_x=row['variance_c'], variance_y=row['variance_t'],\n",
    "      count_x=row['count_c'], count_y=row['count_t'],\n",
    "      theta_0=0, tau_sq=0.0001),\n",
    "    axis=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9347569b-2dcd-424e-aa23-967d5dfa7ae6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "abtest_metrics_df = abtest_metrics_df.sort_values(['experiment_id', 'variant_id', 'metric_id'])\n",
    "\n",
    "abtest_metrics_df['mSPRT-vanilla-p-value'] = (\n",
    "  abtest_metrics_df\n",
    "  .groupby(['experiment_id', 'variant_id', 'metric_id'])\n",
    "  ['mSPRT-vanilla-p-value-aux']\n",
    "  .transform(lambda row: row.expanding(min_periods=2).min())\n",
    ")\n",
    "\n",
    "abtest_metrics_df['mSPRT-vanilla-p-value'] = (\n",
    "  abtest_metrics_df['mSPRT-vanilla-p-value'].fillna(1.0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6dcdda9-c64d-4fd9-a8e0-541b97a5226f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "abtest_metrics_df = abtest_metrics_df.merge(abtest_metrics_df.groupby(['experiment_id', 'variant_id', 'metric_id'])\\\n",
    "                                            ['time_since_start'].max().reset_index()\\\n",
    "                                            .rename(columns={'time_since_start':'design_duration'}),on=['experiment_id', 'variant_id', 'metric_id'])\n",
    "abtest_metrics_df['time_progress'] = abtest_metrics_df['time_since_start']/abtest_metrics_df['design_duration']\n",
    "abtest_metrics_df['experiment_variant_id'] = abtest_metrics_df['experiment_id'].map(str) + '-' + abtest_metrics_df['variant_id'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abtest_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca445384-7f09-4741-88a9-458610597fc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"time_progress\", y=\"mSPRT-vanilla-p-value\",\n",
    "             hue=\"experiment_variant_id\",\n",
    "             data=abtest_metrics_df[(abtest_metrics_df.experiment_variant_id.isin(['c56288-1', 'a4386f-1', 'bac0d3-1', '08bcc2-1', '591c2c-1'])) & (abtest_metrics_df.metric_id==1)])\n",
    "\n",
    "plt.savefig(figures_folder + 'time_progress_significance_metric_' + str(metric_id) + '.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "abtest_metrics_df['Both significant']=(abtest_metrics_df['t-test-p-value']<=alpha) & \\\n",
    "    (abtest_metrics_df['mSPRT-vanilla-p-value']<=alpha)\n",
    "abtest_metrics_df['Only mSPRT significant']=(abtest_metrics_df['t-test-p-value']>alpha) & \\\n",
    "    (abtest_metrics_df['mSPRT-vanilla-p-value']<=alpha)\n",
    "abtest_metrics_df['Both not significant']=(abtest_metrics_df['t-test-p-value']>alpha) & \\\n",
    "    (abtest_metrics_df['mSPRT-vanilla-p-value']>alpha)\n",
    "abtest_metrics_df['Only t-test significant']=(abtest_metrics_df['t-test-p-value']<=alpha) & \\\n",
    "    (abtest_metrics_df['mSPRT-vanilla-p-value']>alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abtest_metrics_df[(abtest_metrics_df['time_progress']==1)].groupby('metric_id')[\n",
    "    ['Both significant','Only mSPRT significant','Both not significant','Only t-test significant']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 10\n",
    "labels=['Both not significant','Only mSPRT significant','Only t-test significant','Both significant']\n",
    "abtest_metrics_df['time_progress_bin'] = pd.cut(abtest_metrics_df['time_progress'], \\\n",
    "                                                np.linspace(0,1,nbins+1),retbins=False)\n",
    "\n",
    "#count snapshots per time_bin\n",
    "abtest_metrics_evol_df = abtest_metrics_df.groupby(['experiment_variant_id','time_progress_bin'])[['time_progress']]\\\n",
    "    .count().groupby('experiment_variant_id').min()\\\n",
    "    .rename(columns={'time_progress':'min_samples_in_time_bin'}).reset_index()\n",
    "\n",
    "#filter experiment_variant_id that have at least one entry per time bin\n",
    "abtest_metrics_evol_df = abtest_metrics_evol_df[\n",
    "    abtest_metrics_evol_df['min_samples_in_time_bin']>=1][['experiment_variant_id']]\n",
    "\n",
    "\n",
    "print('#experiments with at least one entry per time bin', len(abtest_metrics_evol_df['experiment_variant_id'].unique()))\n",
    "abtest_metric_evol_df = abtest_metrics_df[['experiment_variant_id','metric_id','time_progress_bin']+ labels]\\\n",
    ".merge(abtest_metrics_evol_df,on='experiment_variant_id')\\\n",
    "    .groupby(['time_progress_bin','experiment_variant_id','metric_id']).last().reset_index()\n",
    "abtest_metric_evol_df['time_progress_bin'] = [x.mid for x in abtest_metric_evol_df['time_progress_bin'].values]\n",
    "\n",
    "abtest_metric_evol_df = abtest_metric_evol_df.groupby(['time_progress_bin','metric_id']).mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abtest_metric_evol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_id in abtest_metric_evol_df['metric_id'].unique():\n",
    "    print('metric_id', metric_id)\n",
    "    x=abtest_metric_evol_df['time_progress_bin'].unique()\n",
    "    y = abtest_metric_evol_df[abtest_metric_evol_df['metric_id']==metric_id][labels].T.values\n",
    "\n",
    "    # set seaborn style\n",
    "\n",
    "    # Plot\n",
    "    plt.stackplot(x,y, labels=labels)\n",
    "    plt.ylim(0,1)\n",
    "\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.show()\n",
    "    plt.savefig(figures_folder + 'time_progress_type_metric_' + str(metric_id) + '.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1c21a533-2d5a-4b93-b4d7-45eb8cb673cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Confusion matrix between Welch's t-test and mSPRT  \n",
    "Confusion matrix between Welch's t-test and Bayesian A/B test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a82c18a8-eb8a-4f99-9999-726e99e9f920",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "cumulative plot off type-I, type-II and correct rejection or no-rejection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "430f2640-5768-4455-9e3e-11c7f237f57b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Show data spike experiment, comment on how it affects mSPRT(vanilla), mSPRT(in deployment) and Bayesian A/B Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bd5f9d78-8cf4-4e80-a80b-4f2b95fcce0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "TODO: use markdown instead of cell title for export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1b47f91a-ff13-45e6-9225-888932e544b3",
     "showTitle": true,
     "title": "Perform mSPRT on dataset"
    }
   },
   "source": [
    "\n",
    "TODO: TODO: Fix effective sample size n\n",
    "TODO: estimate sigma^2 via whatever means"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "oce-dataset",
   "notebookOrigID": 1499717207767751,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
